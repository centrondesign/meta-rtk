From d73123a6fac684657c259c27bcabd2900f1a3e7c Mon Sep 17 00:00:00 2001
From: Charles Lee <charles.lee@realtek.com>
Date: Tue, 27 May 2025 02:44:56 +0000
Subject: [PATCH 7/8] Add pixel format into input option

---
 .../face-processing/common/facedetectpipe.py  | 22 ++++++++++---------
 .../example_face_recognition_tflite.py        |  6 ++++-
 2 files changed, 17 insertions(+), 11 deletions(-)

diff --git a/tasks/face-processing/common/facedetectpipe.py b/tasks/face-processing/common/facedetectpipe.py
index 00983d2..ed00ee2 100644
--- a/tasks/face-processing/common/facedetectpipe.py
+++ b/tasks/face-processing/common/facedetectpipe.py
@@ -117,7 +117,8 @@ class SecondaryPipe(Pipe):
     def __init__(self,
                  secondary_model,
                  video_resolution=(640, 480),
-                 video_fps=30):
+                 video_fps=30,
+                 format='NV16'):
         """Secondary GStreamer pipeline applying 2nd model on detected faces.
 
         Secondary GStreamer pipeline to be fed with video stream and a set of
@@ -141,22 +142,21 @@ class SecondaryPipe(Pipe):
 
         self.processing_complete_cb = None
 
+        iomode = 4
+
         # secondary pipeline (face crop + second model) GStreamer definition
         video_caps = ('video/x-raw,'
-                      'width={:d},height={:d},framerate={:d}/1,format=NV12') \
-            .format(vw, vh, vr)
+                      'width={:d},height={:d},framerate={:d}/1,format={:s}') \
+            .format(vw, vh, vr, format)
 
         cmdline = 'appsrc name=appsrc_video is-live=true caps={:s} format=3 ' \
             .format(video_caps)
-        cmdline += '    emit-signals=false max-buffers=1 leaky_type=2 ! '
+        cmdline += '  emit-signals=false max-buffers=1 leaky_type=2 ! '
         cmdline += '  {:s} ! '.format(video_caps)
 
         h, w, num_channels = secondary_model.get_model_input_shape()
-        if num_channels == 3:
-            format = 'RGB'
-        elif num_channels == 1:
-            format = 'GRAY8'
-        cmdline += ' v4l2convert name=video_crop output-io-mode=4  extra-controls="controls,npp_single_rgb_plane=1" ! video/x-raw,width=160,height=160 ! '
+
+        cmdline += ' v4l2convert name=video_crop output-io-mode={:d}  extra-controls="controls,npp_single_rgb_plane=1" ! video/x-raw,width=160,height=160 ! '.format(iomode)
 
         cmdline += ' tensor_converter !  '
         cmdline += (' tensor_filter '
@@ -313,6 +313,8 @@ class FaceDetectPipe(Pipe):
 
         ufh, ufw, _ = self.ultraface.get_model_input_shape()
 
+        iomode = 4
+
         if codec == 'H264':
             video_caps = ('video/x-h264, '
                         'width={:d},height={:d},framerate={:d}/1') \
@@ -333,7 +335,7 @@ class FaceDetectPipe(Pipe):
         cmdline += ' tee name=tvideo '
 
         cmdline += ' tvideo. ! queue max-size-buffers=1 leaky=2 ! '
-        cmdline += ' v4l2convert  output-io-mode=4 extra-controls="controls,npp_single_rgb_plane=1" ! video/x-raw,width=320,height=240,colorimetry=BT.709 ! '
+        cmdline += ' v4l2convert  output-io-mode={:d} extra-controls="controls,npp_single_rgb_plane=1" ! video/x-raw,width=320,height=240 ! '.format(iomode)
         #cmdline += ' videoconvert ! videoscale ! video/x-raw,width=320,height=240,format=RGB ! '
         cmdline += ' tensor_converter ! tensor_transform mode=transpose option=1:2:0:3 ! tensor_transform mode=typecast option=uint8 !  '
 
diff --git a/tasks/face-processing/face-recogniton/example_face_recognition_tflite.py b/tasks/face-processing/face-recogniton/example_face_recognition_tflite.py
index 74f9a0e..2b31bcc 100644
--- a/tasks/face-processing/face-recogniton/example_face_recognition_tflite.py
+++ b/tasks/face-processing/face-recogniton/example_face_recognition_tflite.py
@@ -356,6 +356,8 @@ if __name__ == '__main__':
                         help='display window size, e.g. --window=640,480 or --window="(640,480)"')
     parser.add_argument('--codec', '-k', type=str, choices=['H264', 'MJPEG'],
                         default='MJPEG', help='video codec to be used, default=MJPEG')
+    parser.add_argument('--format', type=str, choices=['NV12', 'NV16'],
+                        default='NV16', help='YUV pixel formats of camera output, default=NV16')
     args = parser.parse_args()
 
     format = '%(asctime)s.%(msecs)03d %(levelname)s:\t%(message)s'
@@ -370,6 +372,7 @@ if __name__ == '__main__':
     pos = args.position if args.position else (0, 0)
     win = args.window if args.window else vr
     codec = args.codec.upper()
+    format = args.format.upper()
 
     if win[0] > vr[0] or win[1] > vr[1]:
         parser.error(f"Window size must be no bigger than resolution, got {win}")
@@ -380,7 +383,8 @@ if __name__ == '__main__':
     mface_db_dir = os.path.join(pwd, 'facenet_db')
 
     model = facenet.FNModel(models_dir, mface_db_dir)
-    secondary = SecondaryPipe(model, video_resolution=vr, video_fps=fps)
+    secondary = SecondaryPipe(model, video_resolution=vr, video_fps=fps,
+                              format=format)
 
     # main pipeline for face detection
     pipe = FaceRecoPipe(camera_device=camera_device, video_resolution=vr,
-- 
2.25.1

